---
title: "visualizemi Tutorial"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{visualizemi Tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: references.bib
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(visualizemi)
library(lavaan)
library(knitr)
library(ggplot2)
library(introdataviz)
library(ggridges)
```

## Multigroup analysis

If you are interested in measurement invariance, you would likely want to conduct a multigroup factor analysis (MGCFA) to compare two or more groups on proposed model structure. This generally consists of steps of models with increasing restrictions between groups to constrain parameters to be equal across groups. Invariance is achieved by showing that two groups have equal values on the model, and non-invariance occurs when they do not show the same values. Check out excellent book chapters by @brown2015 and @kline2016 to learn more about theoretical backgrounds and ideas behind MGCFA.

In *R*, MGCFA can be achieved by using `lavaan` - a structural equation modeling package that covers a large range of possible modeling options [@rosseel2012]. Their tutorial on multigroups can be found at: <https://lavaan.ugent.be/tutorial/groups.html>. `visualizemi` includes a MGCFA convenience function that flexibly calculates the following:

1) An overall model without groups
2) Individual group based models
3) A "configural" model wherein both groups are put together into one model with no constraints
4) User entered constraint based models 

Additionally, the function returns a list of coefficients and fit statistics for comparison using your favorite rules for determining measurement invariance. 

```{r}
HS.model <- ' visual  =~ x1 + x2 + x3
              textual =~ x4 + x5 + x6
              speed   =~ x7 + x8 + x9 '

                    # cfa model
saved_mgcfa <- mgcfa(model = HS.model,
                     # dataset in data frame 
                     data = HolzingerSwineford1939, 
                     # grouping variable column 
                     group = "sex",
                     # lavaan syntax for group constraints
                     group.equal = c("loadings", "intercepts", "residuals"), 
                     # any other lavaan cfa arguments
                     meanstructure = TRUE)

# note you can also include sample.nobs, sample.cov, and sample.mean if 
# you only have the correlation or covariance matrices
```

These outputs get very long, so here's a summary of what's available - models are full `lavaan` model structure, so you can use `summary()`, `parameterEstimates()`, `fitmeasures()`, and so on with them. They are commented out to save space in this tutorial. 

```{r}
kable(head(saved_mgcfa$model_coef))

kable(saved_mgcfa$model_fit)

# overall
saved_mgcfa$model_overall

# groups
# saved_mgcfa$group_models$model.1
# saved_mgcfa$group_models$model.2

# configural
# saved_mgcfa$model_configural

# constraints
# saved_mgcfa$invariance_models$model.loadings
# saved_mgcfa$invariance_models$model.intercepts
# saved_mgcfa$invariance_models$model.residuals
```

## Partial invariance

Partial invariance occurs when *most* of the parameter estimates are the same across groups but not all of them. `visualizemi` includes a useful function to investigate which items are non-invariant. If I use one of the more popular rules of them for invariance - $\Delta$CFI <= .01 - then I could decide that the loadings are not equal across groups in comparison to the configural model. I would use the `partial_mi()` function to figure out how the items change the model with their constraint is relaxed individually across groups. You get a table of fit indices to review for your favorite invariance rules and each model saved separately to determine partial invariance. 

```{r}
                            # a saved model from mgcfa or any lavaan model
saved_partial <- partial_mi(saved_model = saved_mgcfa$invariance_models$model.loadings,
                            # dataframe of the original data
                            data = HolzingerSwineford1939, 
                            # model syntax from lavaan
                            model = HS.model, 
                            # group variable column
                            group = "sex",
                            # the equality constraints you have in the model
                            group.equal = c("loadings"),
                            # which step to test the partial invariance on
                            partial_step = "loadings")
# note that the group.equal and partial_step are not always the same 

kable(saved_partial$fit_table)

# saved_partial$models$`visual =~ x1`
# saved_partial$models$`visual =~ x2`
# and so on
```

In our case, one parameter `speed =~ x9` clearly can improve the overall model to be equal to the original configural model. I could then keep exploring steps and/or partial invariance for other constraints. 

```{r}
saved_mgcfa.partial <- mgcfa(model = HS.model,
                     # dataset in data frame 
                     data = HolzingerSwineford1939, 
                     # grouping variable column 
                     group = "sex",
                     # lavaan syntax for group constraints
                     group.equal = c("loadings", "intercepts", "residuals"), 
                     # any other lavaan cfa arguments
                     meanstructure = TRUE,
                     group.partial = c("speed =~ x9")
)

kable(saved_mgcfa.partial$model_fit)
```

## Plotting MI

The package then includes a visualization tool for the measurement invariance. Plots are parameter specific, so we could examine `speed =~ x9` for the strength and direction of the invariance. 

```{r}
                          # a table of tidy coefficients, use broom::tidy 
                          # and create "model" column if you don't use mgcfa
                          # be sure to use the partial model or one with out 
                          # constraints or this graph will be boring
saved_mi_plots <- plot_mi(data_coef = saved_mgcfa.partial$model_coef,
                          # which model do you want to plot from model column
                          model_step = "loadings", 
                          # name of observed item
                          item_name = "x9", 
                          # LV limits to graph
                          x_limits = c(-1,1), 
                          # Y min and max in data
                          y_limits = c(min(HolzingerSwineford1939$x9),
                                       max(HolzingerSwineford1939$x9)), 
                          # what ci do you want
                          conf.level = .95, 
                          # what model results do you want
                          model_results = saved_mgcfa.partial$invariance_models$model.loadings, 
                          # which latent is the observed variable on
                          # important for cross-loaded variables
                          lv_name = "speed", 
                          # if you have more than two groups, which two do you want
                          plot_groups = NULL)

saved_mi_plots$complete
```

The plots are explained at length in the preprint: https://osf.io/9hzfe/ but short version:

Observed variable information:

  1) Intercepts are plotted on the left side as the dots on the y-axis
  2) Loadings are plotted as theh slope of the lines on the left hand side
  3) Residuals are plotted as the geom violin on the left hand side
  
Latent variable information:

  4) LV distribution is plotted on the right hand side
  5) LV means are represented as the solid lines for each group 

This plot is the combination of three smaller plots, and they are included separately so you can modify them with `ggplot2` if you like. We can clearly see that group 1 has a shallower slope (i.e., a smaller factor loading) than group 2. 

## Pre-registration, replication, and registered reports

Several unanswered questions:

  1) How big is this effect? 
    - We could use $d_{macs}$, however this statistic is based on intercepts *and* loadings simultaneously. 
  2) How much should I expect this model to replicate? 
    - If I believe that this data is truly representative of the population, how does this model replicate in comparison to a random model? 

This information would give you an idea of what to expect in a potential replication and/o pre-registration. 

### Replication at model level 

Here we will bootstrap the mgcfa procedure to determine the "replication rate" if our data was representative of the true population. The data is bootstrapped and the steps for MGCFA are examined for invariance. This model is compared to a model of the bootstrapped data with random group assignment. 

```{r}
saved_boot_model <- bootstrap_rr(
  # saved configural model to start at
  saved_configural = saved_mgcfa$model_configural,
  # dataset for the analysis
  data = HolzingerSwineford1939,
  # model lavaan syntax
  model = HS.model,
  # group variable in the dataset
  group = "sex", 
  # nubmer of bootstraps, 100 to make this run faster for examples
  nboot = 50,
  # name of the fit measure you want to use, make sure it's lavaan
  invariance_index = "cfi",
  # rule for the difference in fit indices
  invariance_rule = .01,
  # what order of steps do you want to test? 
  group.equal = c("loadings", "intercepts", "residuals")
)

kable(saved_boot_model)
```

This data tells you the proportion of values that were non-invariant for the bootstrapped and random models. Note: this may not add up to `n_boot` because sometimes models fail to converge. `h` represents the effect size comparison of the two proportions (much like a $d$ value). We can see that it's likely that our model would not show invariance for all parameters compared to a random model for the loadings. 

### Size of the effect and replication at parameter level

In this function, we will bootstrap our model (up to the step we claim partial invariance) to determine our replication rate and effect size. Note: this function is slow depending on the number parameters, bootstraps, and computer size. 

1) The saved dataframe of the estimates for each group's *standardized* loading using `std.all` for the original model and the random model. This value is like a $\beta$ or effect size - so we can use it to determine what the effect size of the difference would be if they were constrained to equal or not. This dataframe is included if you want to make your own graphs.
2) The summary dataframe of the bootstrapped results. Note: you will see NAs when you do not have enough values that fall into the "invariant" or "non-invariant" separation. Effect sizes are calculated with `calculate_d()` assuming between subjects grouping (group 1 versus group 2) using the average standardized parameter as the mean and the average standard deviation of that parameter as the standard deviation. A minimum of 10% of bootstraps is required for the effect size to calculate. 
3) A forest plot style graph of the mean group differences. 
4) A forest plot style graph of the effect sizes of the group differences with emphasis on the number of bootstraps included in each calculation. 

Note: invariance is FALSE for non-invariant, TRUE for invariant using: `(original model fit - model relaxed fit) <= rule`

The last three are ggplot objects, so you can edit them from the saved output. 

```{r}
saved_boot_partial <- bootstrapped_partial(
  # the model you want to test 
  # use the model before your invariant one 
  # similar set up to partial_mi
  saved_model = saved_mgcfa.partial$model_configural,
  # the dataframe
  data = HolzingerSwineford1939,
  # the original model syntax
  model = HS.model,
  # the grouping variable column
  group = "sex",
  # run more, but this package vignette needs to knit fast
  nboot = 50,
  # what index are you using for invariance?
  # match this to lavaan's name under fitmeasures()
  invariance_index = "cfi",
  # what rule are you using?
  invariance_rule = .01,
  # what are we comparing against? 
  invariance_compare = unname(fitmeasures(saved_mgcfa.partial$model_configural, "cfi")),
  # which step you want to estimate effect size for
  partial_step = c("loadings"), 
  # which parameters you want to hold constrained
  group.equal = c("loadings")
  )

kable(head(saved_boot_partial$boot_DF))

kable(saved_boot_partial$boot_summary)

kable(saved_boot_partial$boot_effects)

saved_boot_partial$invariance_plot

saved_boot_partial$effect_invariance_plot

saved_boot_partial$density_plot
```

I can see very few models are considered "invariant" when individually relaxing the parameters. These three parameters are the same ones that showed as the top items to relax to create an invariant model BUT that doesn't bootstrap consistently, so it may not replicate. The randomly assigned group models show an even split of replication, which you would expect, given it's "random". 

The boot effects dataframe gives me a potential size of the non-invariant effect compared to random, illuminating the items that may not replicate, and my boot summary gives me effect sizes differences between groups for bootstrapped and random data. I could subtract these differences to tell how big one might expect them to be as compared to random or simply review them in my invariance plots. 

